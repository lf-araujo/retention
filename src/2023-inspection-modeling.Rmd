---
title: "Inspecting data and initial modeling attempts"
subtitle: "ABCD retention WG"
author: "Luis Castro-de-Araujo ^[Post-doc T32. luis.araujo@vcuhealth.org  \n]"
date: "`r format(Sys.time(), '%d %B, %Y' )`"
institute: Virginia Institute for Psychiatric and Behavioral Genetics 
lang: en
output:
  html_notebook:
    code_folding: hide
    highlight: zenburn
    theme: flatly
    toc: yes
    df_print: paged    
    toc_float: yes
    code_downloading: yes
---


```{r setup, include=FALSE}

c( # I use linux, this saves my time loading/installing packages
  "ProjectTemplate", "here", "stringr",
  "dplyr", "knitr", "ggplot2", "tabplot",
  "patchwork", "tableone", "tidyr", "visdat", "glmnet",
  "doParallel", "kableExtra", "dtplyr"
) |>
  lapply(function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x)
    }
  })

set.seed(42) # setting seed for stochastic functions
setwd("..") # here()) # needed as we are in /src, in linux here() should be used

load.project() # Loading the project
pclean()

# R options
options(
  digits = 3, # Only two decimal digits
  scipen = 999, # Remove scientific notation
  width = 100
)


# Knitr options
knitr::opts_chunk$set(
  comment = NA, # remove comment symbol
  cache.path = "../cache/", # where should I save cache?
  fig.path = "../graphs/", # where should I save figures?
  echo = T, # dont echo by default
  cache = F, # dont cache by default
  fig.width = 10, # setting the best witdth for figures
  fig.height = 7, # best height
  dpi = 300, # high dpi for publication quality,
  error = FALSE # do not interrupt in case of errors
)


cb_palette <- c(
  "#999999", "#E69F00", "#56B4E9", "#009E73",
  "#F0E442", "#0072B2", "#D55E00", "#CC79A7"
)

theme_luis <- function() {
  return_theme <- ggplot2::theme_bw(12) +
    ggplot2::theme(
      panel.border = element_rect(colour = "black"),
      legend.background = element_rect(linetype = 1, size = 0.2, colour = 1)
    )
}

# register parallel backend
cl <- makeCluster(detectCores(), outfile = "")
registerDoParallel(cl)
```


```{r munge}

retention  <- read.csv("~/Downloads/dataIndiv_ID_2023-03-08.csv") %>%
  filter(event == "Baseline" | event == "1 Year") %>%
  select(pguid, event, visit_status_aggr, income_household, site, race_ethnicity, education) %>%
  mutate(missed = ifelse(visit_status_aggr == "Missed (explained)" |
                visit_status_aggr == "Missed (unexplained)", 1, 0),
  site = as.factor(site))

# below is a version with wide format, which is the correct way, but there is an oddity in
# the data that the values for some demographic info only appear at the follow up, not at the baseline
# so I suppose the data was joined incorrectly.

# retention_wide <- dataIndiv_ID_2023.03.08 %>%
#   select(pguid, event, visit_status_aggr, income_household, site, race_ethnicity, education) %>%
#   group_by(pguid) %>%
#   filter(event == "Baseline" | event == "1 Year") %>%
#   mutate(missed = ifelse(visit_status_aggr == "Missed (explained)" |
#                 visit_status_aggr == "Missed (unexplained)", 1, 0)) %>%
#   pivot_wider(names_from = event, values_from = c(missed, income_household, site, race_ethnicity, education)) %>%
#   mutate(`missed_1 Year` = ifelse(is.na(`missed_1 Year`), 0, `missed_1 Year`))


# retention %>% 
#   group_by(event) %>% 
#   summarise(n = n(), 
#             n_missed = sum(missed_visit_1y), 
#             perc_missed = round(n_missed/n, 2)) |>
#   kable(align = "c", caption = "Number of participants and percentage of missed visits by event") |>
#   kable_styling()
# # About 5% of participants missed their 1 year visit

```

# Modeling
# Leave-one-out cross validation and coeff


The typical subsetting between test and training return less stable coeffs. Hence, trying LOOCV, which returned more stable results.

- Next step will be to unpack the levels of these ordinal vars. Currently they seem to point to the wrong direction (negative effect on missing a visit).

```{r}


# Set the number of iterations
iterations <- 21  # Assuming there are 21 unique sites

# Initialize empty vectors for storing results
lambda_values <- c()
RMSE_values <- c()
var_imp_avg <- NULL

# Iterate over the sites
for (isite in unique(retention$site)) {
  # Split retention into train and test
  train <- retention %>%
    filter(site != isite)  # Leave out the current site

  test <- retention %>%
    filter(site == isite)  # Use only the current site

  # Prepare dataset in glmnet format for train and test sets
  outcome_train <- data.matrix(train$missed)
  outcome_test <- data.matrix(test$missed)

  predictors_train <- train %>%
    select(income_household,  race_ethnicity, education) %>%
    data.matrix()

  predictors_test <- test %>%
    select(income_household,  race_ethnicity, education) %>%
    data.matrix()

  message("Site: ", isite)

  # Perform cross-validated glmnet model fitting
  cv_model <- cv.glmnet(predictors_train, outcome_train,
                        nfolds = 10,  # Use LOOCV
                family = "binomial",
  parallel =T)

  # Get the best lambda
  best_lambda <- cv_model$lambda.1se

  # Fit the model using the best lambda on the test set
  fit <- glmnet(predictors_test, outcome_test,
                alpha = 1,
                type.measure = "mse",
                lambda = best_lambda,
                family = "binomial",
                intercept = FALSE)

  # Extract the coefficients
  var_imp <- coef(fit, s = fit$lambda.1se) %>%
    as.matrix() %>%
    as.data.frame() %>%
    rownames_to_column("Factor") %>%
    rename("Importance" = "s0")

  # Store the results
  RMSE <- cv_model$cvm[cv_model$lambda == cv_model$lambda.1se] %>% sqrt() %>% round(2)
  lambda <- cv_model$lambda.1se %>% round(2)

  if (is.null(var_imp_avg)) {
    var_imp_avg <- var_imp
  } else {
    var_imp_avg <- left_join(var_imp_avg, var_imp, by = c("Factor"))
    lambda_values <- lambda_values %>% append(lambda)
    RMSE_values <- RMSE_values %>% append(RMSE)
  }
}


upper = c()
lower = c()

for (row in seq_len(nrow(var_imp_avg))){
  upper[row] = sort(var_imp_avg[row,])[round(iterations*0.975,0)] # 97.5th
  lower[row] = sort(var_imp_avg[row,])[round(iterations*0.25,0)] # 25th
}



# get the first column as rownames
var_imp_avg <- column_to_rownames(var_imp_avg, var = "Factor")
# now create a mean per row:
var_imp_avg <- rowMeans(var_imp_avg)
# format as dataframe
var_imp_avg <- as.data.frame(var_imp_avg)
# get rownames back as column
var_imp_avg <- rownames_to_column(var_imp_avg, var = "Factor")
# rename the Importance column
var_imp_avg <- var_imp_avg %>%
  rename(Importance = var_imp_avg)

# round the confidence intervals
upper <- unlist(upper) %>% round(2)
lower <- unlist(lower) %>% round(2)

# create CI column
var_imp_avg$"95% CI" <- paste("[", lower, ", ", upper, "]", sep = "")

# filter out predictors with Variable importance greater than zero
var_imp_avg <- var_imp_avg %>%  
  filter(abs(Importance) > 0)

var_imp_avg %>% 
  #mutate(Importance = round(Importance, 2)) %>% 
  kableExtra::kbl(booktabs = T,
                  col.names = c("Factor", "mean coeff.", "95% CI"),
                  escape = TRUE,
                  format = "markdown")



```

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.081|[-0.05, 0]     |
|race_ethnicity   |      -0.572|[-0.38, -0.92] |
|education        |      -0.123|[-0.09, 0]     |


## GLMnet w lasso 

```{r}

n_folds = 10 # the number of folds for cross validation
var_imp_avg = tibble() # prepare the output dataframe / tibble
iterations = 100 # number of bootstrap samples


for (i in 1:iterations) {
 
  # split retention into train and test
  train <- retention %>% 
    slice_sample(prop=0.8, replace = F) # 20% of the data for training
  test <- retention %>% 
    anti_join(train, by = "pguid") # the rest for testing

  # prepare dataset in glmnet format, and for each train and test sets
  outcome_train <- train$missed %>% 
    data.matrix()
  outcome_test <- test$missed %>%
    data.matrix()

  predictors_train <- train %>%
     select(income_household, site, 
           race_ethnicity, education) %>%
    data.matrix()

  predictors_test <- test %>%
     select(income_household, site, 
           race_ethnicity, education) %>%
    data.matrix()

  cv_model = cv.glmnet(predictors_train, outcome_train, # the formula
                               nfolds = n_folds, # the number of folds in cross validation
                               family = "binomial",
                               parallel = T) 

  # best lambda
  best_lambda <- cv_model$lambda.1se

  fit = glmnet(predictors_test, outcome_test, # the formula
                               alpha = 1, # for lasso
                               type.measure = "mse", # your error measure, here mean squared error
                               lambda = best_lambda, # the best lambda
                               family = "binomial",
                               parallel = T,
                               # type.multinomial = "grouped",
                               intercept = FALSE) # calculate an intercept y/n

  # extract the coefficients
  var_imp <- coef(fit, s = fit$lambda.1se) %>% 
    as.matrix() %>% 
    as.data.frame() %>% 
    rownames_to_column("Factor") %>% 
    rename("Importance" = "s0")
  
  # extract the RMSE and lambda bootstrapped measurements
  RMSE <- cv_model$cvm[cv_model$lambda == cv_model$lambda.1se] %>% sqrt() %>% round(2)
  lambda <- cv_model$lambda.1se %>% round(2)
  
  if(i == 1){
    lambda_values <- lambda
    RMSE_values <- RMSE
    var_imp_avg <- var_imp
  } else{
    var_imp_avg <- left_join(var_imp_avg, var_imp, by = c("Factor"))
    lambda_values <- lambda_values %>% append(lambda)
    RMSE_values <- RMSE_values %>% append(RMSE)
  }
}

upper = c()
lower = c()

for (row in seq_len(nrow(var_imp_avg))){
  upper[row] = sort(var_imp_avg[row,])[round(iterations*0.975,0)] # 97.5th
  lower[row] = sort(var_imp_avg[row,])[round(iterations*0.25,0)] # 25th
}


# get the first column as rownames
var_imp_avg <- column_to_rownames(var_imp_avg, var = "Factor")
# now create a mean per row:
var_imp_avg <- rowMeans(var_imp_avg)
# format as dataframe
var_imp_avg <- as.data.frame(var_imp_avg)
# get rownames back as column
var_imp_avg <- rownames_to_column(var_imp_avg, var = "Factor")
# rename the Importance column
var_imp_avg <- var_imp_avg %>%
  rename(Importance = var_imp_avg)

# round the confidence intervals
upper <- unlist(upper) %>% round(2)
lower <- unlist(lower) %>% round(2)

# create CI column
var_imp_avg$"95% CI" <- paste("[", lower, ", ", upper, "]", sep = "")

# filter out predictors with Variable importance greater than zero
var_imp_avg <- var_imp_avg %>%  
  filter(abs(Importance) > 0)

var_imp_avg %>% 
  #mutate(Importance = round(Importance, 2)) %>% 
  kableExtra::kbl(booktabs = T,
                  col.names = c("Factor", "mean coeff.", "95% CI"),
                  escape = TRUE,
                  format = "markdown")

```

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.075|[-0.05, 0]     |
|site             |      -0.010|[-0.02, 0]     |
|race_ethnicity   |      -0.482|[-0.39, -0.75] |
|education        |      -0.118|[-0.09, -0.2]  |


|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.084|[-0.05, 0]     |
|site             |      -0.009|[-0.02, 0]     |
|race_ethnicity   |      -0.480|[-0.39, -0.74] |
|education        |      -0.119|[-0.09, -0.19] |


|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.069|[-0.05, -0.15] |
|site             |      -0.004|[-0.01, 0]     |
|race_ethnicity   |      -0.400|[-0.36, -0.54] |
|education        |      -0.119|[-0.11, -0.14] |

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.065|[-0.05, -0.1]  |
|site             |      -0.001|[-0.01, 0]     |
|race_ethnicity   |      -0.386|[-0.37, -0.43] |
|education        |      -0.112|[-0.11, -0.13] |

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.060|[-0.05, -0.09] |
|site             |      -0.001|[0, 0]         |
|race_ethnicity   |      -0.384|[-0.37, -0.43] |
|education        |      -0.112|[-0.11, -0.12] |

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.064|[-0.05, -0.09] |
|site             |      -0.001|[0, 0]         |
|race_ethnicity   |      -0.386|[-0.37, -0.42] |
|education        |      -0.111|[-0.11, -0.12] |

|Factor           | mean coeff.|95% CI         |
|:----------------|-----------:|:--------------|
|income_household |      -0.013|[-0.01, 0]     |
|race_ethnicity   |      -0.317|[-0.3, -0.36]  |
|education        |      -0.128|[-0.13, -0.14] | 



# Basic descriptive characteristics

## Distribution of age by sex


Couldn't find the variable with sex in this set

```{r age-by-sex, echo = T}
dataIndiv_ID_2023.03.08 %>%
  ggplot(aes(as.numeric(interview_age), fill = kbisgm_assigned_sex_6yr)) +
  geom_density(alpha = .3) +
  labs(
    title = "Age Male x Female", x = "Age",
    y = "Density"
  ) +
  theme_luis()
```



```{r}
```


# System information

```{r}
project.info
```

```{r}
sessionInfo()
```

# References
